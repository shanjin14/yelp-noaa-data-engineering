{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Restaurant reviews and weather data engineering\n",
    "## Using yelp dataset and NOAA weather API\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this project, we have gather two data sources that have more than 1 million datapoints - yelp and NOAA weather data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### #1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this capstone project, we will be ingesting the yelp review dataset and NOAA weather dataset.\n",
    "\n",
    "##### End Solution Overview\n",
    "The project will follow a two-tier data platform architecture. s3 is used as the Data Lake, Redshift is the Data Warehouse.\n",
    "Data are landed in the raw bucket in s3 and get through transformation and landed as a staging table in Redshift.\n",
    "The data is further transformed into the dimensional model as a foundation data tables\n",
    "<img src=\"DataPlatformArchitecture.PNG\">\n",
    "\n",
    "Benefit of such design is that if we have additional functional area need the same set of data with different dimensional model.\n",
    "We can easily build a separate set of Airflow DAG to address their need without re-loading the data in staging.\n",
    "\n",
    "\n",
    "##### Tools\n",
    "1. Data Lake - s3\n",
    "2. Data Warehouse - Redshift\n",
    "3. ETL Orchestration - Airflow\n",
    "4. ETL processing - EMR (PySpark)\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### 2 data sources:\n",
    "1. NOAA temperature data using NOAA web services API \n",
    "   Constantly extract and load to the datawarehouse\n",
    "   \n",
    "2. Yelp Dataset: The dataset is a sample dataset\n",
    "    However, it can be extended to continuous load if anyone aquire the license from Yelp.\n",
    "    From their developer site, this is something they offered \n",
    "    https://www.yelp.com/developers\n",
    "\n",
    "#### Objective of the dataset\n",
    "1. Explore what are the possible popular location\n",
    "2. What is the effect of weather to the footfall?\n",
    "3. Competition within localitiy location?\n",
    "4. Customer review on the restaurant location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Data overview\n",
    "THe two selected data sources are using json format and API. Both of them have pre-defined schema hence we do not have change of column name so that we can relies on the column name(or key) to do the data processing and load.\n",
    "The key challenge and issue of the two data sources are listed below\n",
    "\n",
    "\n",
    "#### Yelp Dataset \n",
    "##### Data Issue/Challenge #1 - non-atomic value\n",
    "For example in the checkin.json, for each row of business_id, it consist of all the date that users check in single row and it quickly reach the maximum number of character that can be store in redshift in single cell( 65536 characters). In order for it be loaded to redshift, we need to transform the data to row level, which each row has one checkin date.\n",
    "\n",
    "Same issue can be found in users.json, which the friends column consist of all the user's friends user_id and it exceeds the maximum varchar character limit from redshift\n",
    "\n",
    "To resolve the issue, we use pyspark to do a transformation before load the data to redshift\n",
    "\n",
    "##### Data Issue/Challenge #2 timestamp format\n",
    "To ensure the timestamp is correctly parsed into the redshift, as part of pyspark ETL, we parse the timestamp to time stamp data type before load to redshift\n",
    "\n",
    "#### NOAA Weather data\n",
    "##### Data Issue/Challenge #1 - multiple endpoint\n",
    "NOAA has very rich API services. However, the data are located in different endpoints. In order to combine the data into the meaning structure to load to redshift. A python script is written to handle the combination of multiple API endpoints output into single dataframe, create a csv and upload it to s3\n",
    "\n",
    "##### Data Issue/Challenge #2 - missing data\n",
    "NOAA has missing data for some station ID. Error handling is added to exclude those missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### List of scripts on exploring and processing the data:\n",
    "1. WealtherDataProfiling.ipynb , YelpDataProfiling.ipynb - these two are the data exploration done to understand the data schema and the extraction mechanism\n",
    "\n",
    "2. raw_stg_etl.py - this is the data processing script run in EMR for procesing yelp_checkin and yelp_users\n",
    "\n",
    "3. NOAA_temperature_data.py - it is a list of method to extract the data from NOAA API into csv. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The dimensional model follow the star schema and allows exploratory of the users review with business, user, weather attribute to answer questions, such as:\n",
    "1. Given a hot temperature >35 celsius, do we see a drop in customer review\n",
    "2. which restaurant have more influencer coming?\n",
    "\n",
    "<img src=\"ERDiagram.PNG\">\n",
    "\n",
    "\n",
    "Additional attribute are added such as \"is_fluencer\" in fdn_dim_users using a business logic of more than 50 friend and consider as influencer\n",
    "\n",
    "fdn_dim_weather is meshed using the data from yelp business dataset and NOAA_weather using long lat nearest distance.\n",
    "We used 3 nearest weather station to help populate the weather for each business_id\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "Follow the data platform architecture design above, the steps as follow:\n",
    "1. each data source (jsons or API) are 1-to-1 landed into staging with all columns included\n",
    "2. separate ETL job to transform/move the data into the dimensional model above\n",
    "\n",
    "A total of 4 Airflow DAG are implemented as shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "The logical data flow based on the data model and data platform design as below:\n",
    "<img src=\"logical_data_flow.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### ETL/ELT pipeline can be found in below Airflow Dags:\n",
    "1. Airflow DAG - 1_yelp_data_dag.py -- ETL/ELT pipeline for yelp datasets to staging\n",
    "    - the design is follow truncate and load as it is single json file\n",
    "2. Airflow DAG - 2_NOAA_data_dag.py - ELT pipeline for NOAA weather datasets to staging\n",
    "    - the design is follow daily incremental load due to API return payload limitation. We leverage Airflow capability to backfill the data\n",
    "3. Airflow DAG - 3_stg_fdn_dimensions.py - ELT pipeline for move dimension data from staging to foundation table in dimensional model above\n",
    "    - the design is follow truncate and load given its size\n",
    "4. Airflow DAG - 4_stg_to_fdn_fact.py - ELT pipeline for move fact data from staging to foundation table in dimensional model above\n",
    "    - the design is follow incremental load daily since it has pretty big data inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. NOAA data\n",
    "    - ensure data row is unique based on date,location_id,station\n",
    "2. dimensional model\n",
    "    - dimension tables are not empty after truncate and load\n",
    "    - fact - ensure checkin_count equal to the raw data after transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "The data dictionary file can be found in the file:\n",
    "Data_Dictionary.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Rationale\n",
    "The project has used a combination of ELT +ETL\n",
    "1. If the data can be loaded directly to redshift, we follow the \"ELT\" approach and load it to staging table in redshift and subequently \"T\"ransform them into the designated dimensional model\n",
    "2. If the data cannot be loaded directly, we will first \"T\"ransform the data and load them to staging table and subsquently move them into the dimensional model\n",
    "\n",
    "Based on the data nature, we have adopted below technologies:\n",
    "1. Pyspark with EMR cluster : it is used for ETL process. We also used pyspark notebook to explore the dataset as the yelp data is too big to be loaded to single laptop for EDA\n",
    "2. Redshift                 : it is used as the data warehouse(sink) for the staging layer and foundation layer\n",
    "3. Airflow                  : It is used for orchestrating the entire ETL/ELT pipeline. It also handle the incremental load and backfill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Data frequency\n",
    "1. Yelp data - It is designed to do incremental load. We recommend a daily or hourly load based on the data volume , depends on the data you acquire from yelp\n",
    "2. NOAA data - It is designed as daily load job. The pipeline will do incremental load and backfill the past 3 days data in case the pipeline break in any day and still can be recovered in the next day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Scenario planning\n",
    "###### 1. The data was increased by 100x.\n",
    "1. Re-assess the redshift data space and whether we need to scale-up the redshift nodes to have more nodes (and hence more data space)\n",
    "2. Re-assess the airflow timeout parameter and data partitioning:\n",
    "    - We have incorporated timeout parameter to prevent pipeline run indefintely and block the thread. This would need to be re-assessed if need to be changes\n",
    "    - The data is partitioned by day now. However,if the data is 100x more, we might need to change it to hourly job\n",
    "\n",
    "###### 2. The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "1. We would need to schedule a pipeline to run daily before midnight\n",
    "2. Another key consideration is how many day lag allowed by the dashboard. If the user allow the dashboard to have data cutoff at T-2 date. We can incorporate quality checks to ensure the data shown are accurate before it is seen in T date.\n",
    "\n",
    "###### 3. The database needed to be accessed by 100+ people.\n",
    "1. Re-assess the number of nodes available in redshift and if we need to scale it up to handle the load\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string = 'postgresql://dwhuser:Passw0rd@dwhcluster.chznvhx0cy9n.us-west-2.redshift.amazonaws.com:5439/dwh'\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example query to use the dimension model\n",
    "return 5 reviews on the date of 27-December-2020. \n",
    "see what is the weather at that time,business name , if the user is a influencer, and when the user start using yelp\n",
    "* NOTES: i truncate the review using left(text,30) so that it can be presented nicely in the table below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chznvhx0cy9n.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "5 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>date</th>\n",
       "        <th>business_id</th>\n",
       "        <th>user_id</th>\n",
       "        <th>review_id</th>\n",
       "        <th>useful</th>\n",
       "        <th>cool</th>\n",
       "        <th>funny</th>\n",
       "        <th>text</th>\n",
       "        <th>business_name</th>\n",
       "        <th>restaurantspricerange2</th>\n",
       "        <th>average_temp</th>\n",
       "        <th>prcp</th>\n",
       "        <th>is_influencer</th>\n",
       "        <th>yelping_since</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2020-12-27 00:00:00</td>\n",
       "        <td>9M1XZ0637bZpG8SqvgzqrQ</td>\n",
       "        <td>Z9Uy6ftpOus1EJdWmGhLKA</td>\n",
       "        <td>2WMKv0J3z_ng1VwDqD1oWw</td>\n",
       "        <td>0</td>\n",
       "        <td>0</td>\n",
       "        <td>0</td>\n",
       "        <td>First off, I would like to apo</td>\n",
       "        <td>Wisteria</td>\n",
       "        <td>3</td>\n",
       "        <td>4.9</td>\n",
       "        <td>0.0</td>\n",
       "        <td>1</td>\n",
       "        <td>2009-06-14 23:53:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2020-12-27 00:00:00</td>\n",
       "        <td>M2EwmoRKOUUhFjWshPjdjA</td>\n",
       "        <td>C2B5PcCA9TwxwjR6JjZMug</td>\n",
       "        <td>vZB8vpvpRGsshFrYeId3MQ</td>\n",
       "        <td>1</td>\n",
       "        <td>2</td>\n",
       "        <td>1</td>\n",
       "        <td>Great coffee spot sandwiched b</td>\n",
       "        <td>Spokesman - Highland</td>\n",
       "        <td>None</td>\n",
       "        <td>13.8</td>\n",
       "        <td>0.0</td>\n",
       "        <td>1</td>\n",
       "        <td>2013-01-21 06:25:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2020-12-27 00:00:00</td>\n",
       "        <td>xuM7sO0Qea_gt3ww9ZiIew</td>\n",
       "        <td>qsCKYoPIciyJtwIaHQGYQA</td>\n",
       "        <td>8oJ0HjC-iOj7y_yc5r_8bw</td>\n",
       "        <td>0</td>\n",
       "        <td>1</td>\n",
       "        <td>0</td>\n",
       "        <td>Mxed feelings about this. They</td>\n",
       "        <td>Fat Dragon</td>\n",
       "        <td>2</td>\n",
       "        <td>13.8</td>\n",
       "        <td>0.0</td>\n",
       "        <td>1</td>\n",
       "        <td>2007-08-20 17:00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2020-12-27 00:00:00</td>\n",
       "        <td>Vz0k0EGElVGiesGOatEIoQ</td>\n",
       "        <td>wOA30Ot79lxU0xk5IJ8PwQ</td>\n",
       "        <td>CjrszF-K6BsS1G4qIuUIow</td>\n",
       "        <td>0</td>\n",
       "        <td>0</td>\n",
       "        <td>0</td>\n",
       "        <td>Consistently just THE BEST. Th</td>\n",
       "        <td>Conans Pizza South</td>\n",
       "        <td>2</td>\n",
       "        <td>13.8</td>\n",
       "        <td>0.0</td>\n",
       "        <td>0</td>\n",
       "        <td>2011-01-13 02:09:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2020-12-27 00:00:00</td>\n",
       "        <td>iI4bRxfKaAZuYW7lFhceig</td>\n",
       "        <td>9lXHh5TCGqiLHBHg7cOZmQ</td>\n",
       "        <td>ikGVZfIwiHWZtGQq69xzJw</td>\n",
       "        <td>1</td>\n",
       "        <td>1</td>\n",
       "        <td>0</td>\n",
       "        <td>You enter from the Carousel si</td>\n",
       "        <td>Disney&#x27;s Days of Christmas</td>\n",
       "        <td>2</td>\n",
       "        <td>9.6</td>\n",
       "        <td>0.0</td>\n",
       "        <td>0</td>\n",
       "        <td>2010-06-22 23:02:12</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(datetime.datetime(2020, 12, 27, 0, 0), '9M1XZ0637bZpG8SqvgzqrQ', 'Z9Uy6ftpOus1EJdWmGhLKA', '2WMKv0J3z_ng1VwDqD1oWw', 0, 0, 0, 'First off, I would like to apo', 'Wisteria', '3', 4.9, 0.0, 1, datetime.datetime(2009, 6, 14, 23, 53, 40)),\n",
       " (datetime.datetime(2020, 12, 27, 0, 0), 'M2EwmoRKOUUhFjWshPjdjA', 'C2B5PcCA9TwxwjR6JjZMug', 'vZB8vpvpRGsshFrYeId3MQ', 1, 2, 1, 'Great coffee spot sandwiched b', 'Spokesman - Highland', None, 13.8, 0.0, 1, datetime.datetime(2013, 1, 21, 6, 25, 15)),\n",
       " (datetime.datetime(2020, 12, 27, 0, 0), 'xuM7sO0Qea_gt3ww9ZiIew', 'qsCKYoPIciyJtwIaHQGYQA', '8oJ0HjC-iOj7y_yc5r_8bw', 0, 1, 0, 'Mxed feelings about this. They', 'Fat Dragon', '2', 13.8, 0.0, 1, datetime.datetime(2007, 8, 20, 17, 0, 28)),\n",
       " (datetime.datetime(2020, 12, 27, 0, 0), 'Vz0k0EGElVGiesGOatEIoQ', 'wOA30Ot79lxU0xk5IJ8PwQ', 'CjrszF-K6BsS1G4qIuUIow', 0, 0, 0, 'Consistently just THE BEST. Th', 'Conans Pizza South', '2', 13.8, 0.0, 0, datetime.datetime(2011, 1, 13, 2, 9, 31)),\n",
       " (datetime.datetime(2020, 12, 27, 0, 0), 'iI4bRxfKaAZuYW7lFhceig', '9lXHh5TCGqiLHBHg7cOZmQ', 'ikGVZfIwiHWZtGQq69xzJw', 1, 1, 0, 'You enter from the Carousel si', \"Disney's Days of Christmas\", '2', 9.6, 0.0, 0, datetime.datetime(2010, 6, 22, 23, 2, 12))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "select A.date,A.business_id,A.user_id,A.review_id,A.useful,A.cool,A.funny,left(text,30) text\n",
    ",B.name as business_name,B.RestaurantsPriceRange2,C.average_temp,C.prcp,D.is_influencer,D.yelping_since from fdn_fact_reviews  A\n",
    "join fdn_dim_business B on A.business_id=B.business_id\n",
    "join fdn_dim_weather C on A.date::date =C.date::date and A.business_id=C.business_id\n",
    "join fdn_dim_users D on A.user_id=D.user_id\n",
    "where A.date::date='2020-12-27'\n",
    "limit 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Query to show there is >1 million row of raw data\n",
    "* since the data is landed in staging 1-to-1. We can use the staging table to verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chznvhx0cy9n.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>row_count</th>\n",
       "        <th>min</th>\n",
       "        <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>8635403</td>\n",
       "        <td>2004-10-12 11:14:43</td>\n",
       "        <td>2021-01-28 15:38:54</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(8635403, '2004-10-12 11:14:43', '2021-01-28 15:38:54')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select count(*) row_count,min(date),max(date) from stg_yelp_reviews;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.chznvhx0cy9n.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>row_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2189457</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(2189457,)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select count(*) row_count from stg_yelp_users;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
